{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"QApractice.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNPvIi6w/o0X0buA0F2O6At"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5ltAyML7TSZi"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"KjgTF1l4TmTY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget https://korquad.github.io/dataset/KorQuAD_v1.0_train.json -O KorQuAD_v1.0_train.json\n","!wget https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json -O KorQuAD_v1.0_dev.json"],"metadata":{"id":"i17UL3fpToO1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import json\n","import numpy as np\n","from tqdm import tqdm\n","from pathlib import Path\n","from transformers import BertTokenizerFast\n","import tensorflow as tf\n","\n","def read_squad(path):\n","    path = Path(path)\n","    with open(path, 'rb') as f:\n","        squad_dict = json.load(f)\n","\n","    contexts = []\n","    questions = []\n","    answers = []\n","    for group in squad_dict['data']:\n","        for passage in group['paragraphs']:\n","            context = passage['context']\n","            for qa in passage['qas']:\n","                question = qa['question']\n","                for answer in qa['answers']:\n","                    contexts.append(context)\n","                    questions.append(question)\n","                    answers.append(answer)\n","\n","    return contexts, questions, answers\n","\n","train_contexts, train_questions, train_answers = read_squad('KorQuAD_v1.0_train.json')\n","val_contexts, val_questions, val_answers = read_squad('KorQuAD_v1.0_dev.json')"],"metadata":{"id":"OZBnjPoqTthd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('훈련 데이터의 본문 개수 :', len(train_contexts))\n","print('훈련 데이터의 질문 개수 :', len(train_questions))\n","print('훈련 데이터의 답변 개수 :', len(train_answers))\n","print('테스트 데이터의 본문 개수 :', len(val_contexts))\n","print('테스트 데이터의 질문 개수 :', len(val_questions))\n","print('테스트 데이터의 답변 개수 :', len(val_answers))"],"metadata":{"id":"9vaYkEkoW9m6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('첫번째 샘플의 본문')\n","print('-----------------')\n","print(train_contexts[0])"],"metadata":{"id":"AQBFuYRVXB4i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('첫번째 샘플의 질문')\n","print('-----------------')\n","print(train_questions[0])"],"metadata":{"id":"_X_lEt6jXD4t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('첫번째 샘플의 답변')\n","print('-----------------')\n","print(train_answers[0])"],"metadata":{"id":"rrmSJ9pXXJNj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def add_end_idx(answers, contexts):\n","    for answer, context in zip(answers, contexts):\n","        answer['text'] = answer['text'].rstrip()\n","        gold_text = answer['text'] #질문에 대한 답변 \n","        start_idx = answer['answer_start'] #답변 텍스트가 시작하는 위치 \n","        end_idx = start_idx + len(gold_text) #답변 텍스트가 끝나는 위치 \n","\n","        assert context[start_idx:end_idx] == gold_text, \"end_index 계산에 에러가 있습니다.\" #시작 인덱스와 종료 인덱스가 답변과 일치하지 않으면 오류 발생 \n","        answer['answer_end'] = end_idx #답변 텍스트가 끝나는 위치 삽입 \n","\n","add_end_idx(train_answers, train_contexts)\n","add_end_idx(val_answers, val_contexts)"],"metadata":{"id":"rtxxMwmMXL7g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('첫번째 샘플의 답변')\n","print('-----------------')\n","print(train_answers[0])"],"metadata":{"id":"Fcc0lSTWXTbc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_contexts[0][54:57]"],"metadata":{"id":"k9g_V_P2XVbP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizerFast.from_pretrained('klue/bert-base')\n","\n","train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n","val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)"],"metadata":{"id":"gIsBQKKhXK9C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def add_token_positions(encodings, answers):\n","    start_positions = []\n","    end_positions = []\n","    deleting_list = []\n","\n","    for i in tqdm(range(len(answers))): #답변의 개수만큼 반복\n","      #토큰나이저의 char_to_token 함수는 음절 숫자를 token index로 변환\n","        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start'])) \n","        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n","\n","      #답변이 모델 범위를 초과한 위치에 있을 경우 리스트에서 삭제 \n","        if start_positions[-1] is None:\n","            start_positions[-1] = tokenizer.model_max_length \n","            deleting_list.append(i)\n","\n","        if end_positions[-1] is None:\n","            end_positions[-1] = tokenizer.model_max_length\n","            if i not in deleting_list:\n","              deleting_list.append(i)\n","\n","    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n","    return deleting_list\n"],"metadata":{"id":"Xj9DyesuXszp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["deleting_list_for_train = add_token_positions(train_encodings, train_answers)\n","deleting_list_for_test = add_token_positions(val_encodings, val_answers)"],"metadata":{"id":"n-68q30MgW_D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(train_encodings[0])"],"metadata":{"id":"AMFPrIwtX0JJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def delete_samples(encodings, deleting_list):\n","  input_ids = np.delete(np.array(encodings['input_ids']), deleting_list, axis=0)\n","  attention_masks = np.delete(np.array(encodings['attention_mask']), deleting_list, axis=0)\n","  start_positions = np.delete(np.array(encodings['start_positions']), deleting_list, axis=0)\n","  end_positions = np.delete(np.array(encodings['end_positions']), deleting_list, axis=0)\n","\n","  X_data = [input_ids, attention_masks]\n","  y_data = [start_positions, end_positions]\n","\n","  return X_data, y_data\n"],"metadata":{"id":"5jhcpBf1YHH8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train, y_train = delete_samples(train_encodings, deleting_list_for_train)\n","X_test, y_test = delete_samples(val_encodings, deleting_list_for_test)"],"metadata":{"id":"oRyA-SFuf4fM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TPU 작동을 위한 코드 TPU 작동을 위한 코드\n","resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n","tf.config.experimental_connect_to_cluster(resolver)\n","tf.tpu.experimental.initialize_tpu_system(resolver)"],"metadata":{"id":"P3Gs3gFUYN_P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["strategy = tf.distribute.TPUStrategy(resolver)"],"metadata":{"id":"aqEphOgXaLER"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import TFBertModel\n","#모델생성\n","class TFBertForQuestionAnswering(tf.keras.Model):\n","    def __init__(self, model_name):\n","        super(TFBertForQuestionAnswering, self).__init__()\n","        self.bert = TFBertModel.from_pretrained(model_name, from_pt=True)\n","        self.qa_outputs = tf.keras.layers.Dense(2,\n","                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(0.02),\n","                                                name='qa_outputs') \n","        self.softmax = tf.keras.layers.Activation(tf.keras.activations.softmax)\n","\n","    def call(self, inputs):\n","        input_ids, attention_mask = inputs\n","        outputs = self.bert(input_ids, attention_mask=attention_mask)\n","    \n","        sequence_output = outputs[0]\n","\n","        logits = self.qa_outputs(sequence_output)\n","        start_logits, end_logits = tf.split(logits, 2, axis=-1)\n","\n","        # start_logits = (batch_size, sequence_length,)\n","        # end_logits = (batch_size, sequence_length,)\n","        start_logits = tf.squeeze(start_logits, axis=-1)\n","        end_logits = tf.squeeze(end_logits, axis=-1)\n","\n","        start_probs = self.softmax(start_logits)\n","        end_probs = self.softmax(end_logits)\n","\n","        return start_probs, end_probs\n"],"metadata":{"id":"FbWgGNvmaNG4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 신규\n","with strategy.scope():\n","  model = TFBertForQuestionAnswering(\"klue/bert-base\")\n","  optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n","  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n","  model.compile(optimizer=optimizer, loss=loss)"],"metadata":{"id":"ZunfcHBMaXC9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(\n","    X_train,\n","    y_train,\n","    epochs=3,\n","    verbose=1,\n","    batch_size=16,\n",")"],"metadata":{"id":"N6JlV1iubKml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict_test_data_by_idx(idx):\n","  context = tokenizer.decode(X_test[0][idx]).split('[SEP] ')[0]\n","  question = tokenizer.decode(X_test[0][idx]).split('[SEP] ')[1]\n","  print('본문 :', context)\n","  print('질문 :', question)\n","  answer_encoded = X_test[0][idx][y_test[0][idx]:y_test[1][idx]+1]\n","  print('정답 :',tokenizer.decode(answer_encoded))\n","  output = model([tf.constant(X_test[0][idx])[None, :], tf.constant(X_test[1][idx])[None, :]])\n","  start = tf.math.argmax(tf.squeeze(output[0]))\n","  end = tf.math.argmax(tf.squeeze(output[1]))+1\n","  answer_encoded = X_test[0][idx][start:end]\n","  print('예측 :',tokenizer.decode(answer_encoded))\n","  print('----------------------------------------')"],"metadata":{"id":"uYJiMMx2eCYo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(0, 100):\n","  predict_test_data_by_idx(i)"],"metadata":{"id":"eh-xvWt8bXv4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"HacyuAsPpkh5"},"execution_count":null,"outputs":[]}]}